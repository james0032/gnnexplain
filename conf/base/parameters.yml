# =============================================================================
# GNN Explainer Pipeline Parameters
# =============================================================================

# Base data directory - set via environment variable DATA_DIR
# Default: "data" (relative to project root)
# For server: export DATA_DIR="/projects/aixb/jchung/everycure/influence_estimate/robokop/gnn_clean_baseline/data"
#
# NOTE: Since Kedro doesn't register oc.env by default, we handle DATA_DIR in the code.
# The paths below use "data" as default; if DATA_DIR is set, the code will use it.

# Data paths (relative paths - code will prepend DATA_DIR if set)
data:
  train_file: "data/01_raw/robo_train.txt"
  val_file: "data/01_raw/robo_val.txt"
  test_file: "data/01_raw/robo_test.txt"
  node_dict: "data/01_raw/node_dict"
  rel_dict: "data/01_raw/rel_dict"

# Model architecture hyperparameters
model:
  # Model selection
  model_type: "compgcn"          # Options: "rgcn", "compgcn"
  decoder_type: "conve"        # Options: "distmult", "complex", "rotate", "conve"

  # Common parameters
  embedding_dim: 32             # Embedding dimension (200 for ComplEx/RotatE/ConvE, 128 for DistMult)
  num_layers: 2                  # Number of GNN layers
  dropout: 0.2                   # Dropout rate

  # RGCN-specific parameters
  num_bases: 30                  # Number of bases (RGCN only)

  # CompGCN-specific parameters
  comp_fn: "corr"                 # Composition function: "sub", "mult", "corr"

  # ConvE-specific parameters (only used when decoder_type="conve")
  conve_input_drop: 0.2          # Input dropout
  conve_hidden_drop: 0.3         # Hidden layer dropout
  conve_feature_drop: 0.2        # Feature dropout
  conve_num_filters: 32          # Number of conv filters
  conve_kernel_size: 3           # Kernel size for convolution

# Training configuration
training:
  learning_rate: 0.001
  batch_size: 131072
  num_epochs: 5
  patience: 10
  weight_decay: 0.0
  gradient_clip: 1.0
  checkpoint_interval: 2                  # Save checkpoint every N epochs
  checkpoint_dir: "data/04_model_checkpoints"  # Directory for training checkpoints (or use DATA_DIR env var)

# Evaluation settings
evaluation:
  num_neg_samples: 10
  scoring_batch_size: 8192            # Batch size for compute_test_scores (larger = faster, uses more GPU memory)
  top_k_triples: 100                   # Number of top triples to output to topk_test.txt
  custom_test_file: "/projects/aixb/jchung/everycure/influence_estimate/robokop/gnn_clean_baseline/data/01_raw/matrix.txt"              # Optional: path to custom test file (tab-separated: head\trelation\ttail)
                                      # If set, scores triples from this file instead of default test set
                                      # Example: "data/01_raw/my_triples.txt"
  hit_k_values: [1, 3, 10]
  compute_mrr: true
  compute_hits: true
  batch_size: 1024

# Explanation settings
explanation:
  # Explainer selection - which explainers to run
  # Options: ["gnnexplainer", "pgexplainer", "page", "pagelink"]
  # Set to a list of explainers you want to run, or ["all"] to run all explainers
  # enabled_explainers: ["gnnexplainer", "pgexplainer", "page", "pagelink"]  # Run all explainers
  enabled_explainers: ["pagelink"]
  # Triple selection parameters
  triple_selection:
    strategy: "from_file"              # Options: "random", "test_triples", "from_file", "specific_relations", "specific_nodes"
                                          # - "random": randomly select from training graph edges
                                          # - "test_triples": randomly select from test set triples
                                          # - "from_file": load triples from a file (e.g., topk_test.txt)
                                          # - "specific_relations": select triples with specific relation types
                                          # - "specific_nodes": select triples involving specific nodes
    num_triples: 10                       # Number of triples to explain (ignored for "from_file")
    catalog_name: "top10_test"            # Kedro catalog dataset name (preferred method)
    file_path: "data/07_model_output/top10_test.txt"  # Fallback file path for "from_file" strategy
    target_relations: [0, 1]              # For "specific_relations" strategy
    target_nodes: []                      # For "specific_nodes" strategy

  # GNNExplainer parameters
  gnnexplainer:
    gnn_epochs: 200                       # Optimization epochs for each explanation
    gnn_lr: 0.01                          # Learning rate for mask optimization
    top_k_edges: 10                       # Number of top important edges to extract
    subgraph_method: "paths"               # Options: "khop" (PyG k-hop), "paths" (igraph paths)
    khop_distance: 2                      # For khop method: number of hops
    max_path_length: 3                    # For paths method: maximum path length

  # PGExplainer parameters
  pgexplainer:
    pg_epochs: 30                         # Training epochs for explainer network
    pg_lr: 0.003                          # Learning rate for explainer network
    training_edges: 100                   # Number of edges to train explainer on
    top_k_edges: 10                       # Number of top important edges to extract
    subgraph_method: "paths"              # Options: "khop" (PyG k-hop), "paths" (igraph paths)
    khop_distance: 2                      # For khop method: number of hops
    max_path_length: 3                    # For paths method: maximum path length
    use_full_graph_training: true        # Use full graph for training (slower but comprehensive)
    force_retrain: false                  # Force retraining even if cached explainer exists

  # PAGE parameters (Improved Parametric Generative Explainer)
  # Uses CompGCN embeddings + Prediction-aware training
  page:
    train_epochs: 100                     # Training epochs for VGAE
    lr: 0.003                             # Learning rate
    k_hops: 2                             # Number of hops for subgraph extraction (khop method)
    subgraph_method: "paths"              # Options: "khop" (PyG k-hop), "paths" (igraph paths)
    max_path_length: 3                    # For paths method: maximum path length
    encoder_hidden1: 32                   # Encoder hidden layer 1 dimensions
    encoder_hidden2: 16                   # Encoder hidden layer 2 dimensions (unused in improved version)
    latent_dim: 16                        # Latent space dimensions
    decoder_hidden1: 16                   # Decoder hidden layer 1 dimensions
    decoder_hidden2: 16                   # Decoder hidden layer 2 dimensions (unused in improved version)
    dropout: 0.0                          # Dropout rate
    kl_weight: 0.2                        # Weight for KL divergence loss
    prediction_weight: 1.0                # Weight for prediction-aware training (NEW!)
    top_k_edges: 100                       # Number of top important edges to extract
    checkpoint_interval: 10               # Save checkpoint every N epochs during PAGE training
    checkpoint_dir: "data/06_explainer_cache"  # Directory for PAGE checkpoints
    force_retrain: false                  # Force retraining even if cached model exists
    force_reextract: false                # Force re-extraction of subgraphs even if cached

  # PaGE-Link parameters (Path-based GNN Explanation for heterogeneous Link prediction)
  # Aligned with original: https://github.com/amazon-science/page-link-path-based-gnn-explanation
  pagelink:
    num_epochs: 200                         # Optimization epochs (with early stopping on weight norm convergence)
    lr: 0.001                               # Learning rate for edge mask optimization (Adam)
    alpha: 1.0                              # Weight for on-path loss (encourage high weights on paths)
    beta: 1.0                               # Weight for off-path loss (encourage low weights off paths)
    k_paths: 200                              # Number of top paths to extract via Yen's algorithm
    max_path_length: 3                      # Max path length for path extraction
    top_k_edges: 10000                        # Number of top important edges to extract
    num_hops: 2                             # Number of hops for k-hop subgraph extraction
    prune_max_degree: -1                    # Remove edges of nodes with degree > this (-1 to disable)
    k_core: 2                               # k for k-core subgraph extraction
    checkpoint_dir: "data/06_explainer_cache"  # Directory for PaGE-Link checkpoints

  # Legacy explanation settings (for old explanation pipeline)
  num_explain: 20
  subject_prefixes: ["CHEBI", "UNII", "PUBCHEM.COMPOUND"]
  object_prefixes: ["MONDO"]
  no_prefix_filter: false
  use_fast_explainer: true
  explanation_khops: 2
  max_edges: 2000
  max_path_length: 3

# Visualization settings
visualization:
  explanation_dir: "data/08_reporting/explanations"
  top_k_edges: 20
  figure_size: [15, 10]
  dpi: 300
  node_size: 3000
  font_size: 8
  edge_font_size: 6

# Device configuration
device: "cuda"  # Options: "cuda", "cpu", "mps" (for Mac M1/M2)

# =============================================================================
# MLflow Experiment Tracking Configuration
# =============================================================================
# Controls whether MLflow experiment tracking is enabled
# Set to true to enable MLflow tracking, false to disable (default)
#
# Usage:
#   - Run without MLflow: kedro run
#   - Run with MLflow: kedro run --params=mlflow.enabled:true
mlflow:
  enabled: false                          # Default: disabled (toggle with --params=mlflow.enabled:true)
  tracking_uri: "mlruns"                  # Local tracking directory or remote server URL
  experiment_name: "gnn-explainer"        # Experiment name for organizing runs
