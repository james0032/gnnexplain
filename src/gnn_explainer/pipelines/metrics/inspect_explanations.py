#!/usr/bin/env python3
"""
Script to inspect and analyze pickle files in model explanations directory.

This script helps understand the structure and content of explanation pickle files
generated by the GNN explainer pipeline.
"""

import pickle
import sys
from pathlib import Path
from typing import Any, Dict, List
import numpy as np
import torch


def get_size_mb(obj: Any) -> float:
    """Get approximate size of object in MB."""
    try:
        import sys
        return sys.getsizeof(obj) / (1024 * 1024)
    except:
        return 0.0


def analyze_tensor(tensor: torch.Tensor, name: str = "tensor") -> Dict[str, Any]:
    """Analyze a PyTorch tensor."""
    return {
        "name": name,
        "type": "torch.Tensor",
        "shape": list(tensor.shape),
        "dtype": str(tensor.dtype),
        "device": str(tensor.device),
        "requires_grad": tensor.requires_grad,
        "min": float(tensor.min().item()) if tensor.numel() > 0 else None,
        "max": float(tensor.max().item()) if tensor.numel() > 0 else None,
        "mean": float(tensor.float().mean().item()) if tensor.numel() > 0 else None,
        "num_elements": tensor.numel(),
    }


def analyze_array(arr: np.ndarray, name: str = "array") -> Dict[str, Any]:
    """Analyze a NumPy array."""
    return {
        "name": name,
        "type": "np.ndarray",
        "shape": list(arr.shape),
        "dtype": str(arr.dtype),
        "min": float(np.min(arr)) if arr.size > 0 else None,
        "max": float(np.max(arr)) if arr.size > 0 else None,
        "mean": float(np.mean(arr)) if arr.size > 0 else None,
        "num_elements": arr.size,
    }


def analyze_dict(d: Dict, name: str = "dict", depth: int = 0, max_depth: int = 3) -> Dict[str, Any]:
    """Recursively analyze a dictionary structure."""
    result = {
        "name": name,
        "type": "dict",
        "num_keys": len(d),
        "keys": list(d.keys()),
    }

    if depth < max_depth:
        result["contents"] = {}
        for key, value in d.items():
            result["contents"][key] = analyze_object(value, f"{name}.{key}", depth + 1, max_depth)

    return result


def analyze_list(lst: List, name: str = "list", depth: int = 0, max_depth: int = 3) -> Dict[str, Any]:
    """Analyze a list structure."""
    result = {
        "name": name,
        "type": "list",
        "length": len(lst),
    }

    if len(lst) > 0:
        result["first_element_type"] = type(lst[0]).__name__

        # If it's a list of dicts, analyze first one
        if depth < max_depth and isinstance(lst[0], dict):
            result["sample_element"] = analyze_dict(lst[0], f"{name}[0]", depth + 1, max_depth)
        elif depth < max_depth and isinstance(lst[0], (torch.Tensor, np.ndarray)):
            result["sample_element"] = analyze_object(lst[0], f"{name}[0]", depth + 1, max_depth)

    return result


def analyze_object(obj: Any, name: str = "object", depth: int = 0, max_depth: int = 3) -> Dict[str, Any]:
    """Analyze any Python object."""
    if isinstance(obj, torch.Tensor):
        return analyze_tensor(obj, name)
    elif isinstance(obj, np.ndarray):
        return analyze_array(obj, name)
    elif isinstance(obj, dict):
        return analyze_dict(obj, name, depth, max_depth)
    elif isinstance(obj, list):
        return analyze_list(obj, name, depth, max_depth)
    elif isinstance(obj, (int, float, str, bool, type(None))):
        return {
            "name": name,
            "type": type(obj).__name__,
            "value": obj if not isinstance(obj, (int, float)) or abs(obj) < 1e6 else f"{obj:.2e}",
        }
    else:
        return {
            "name": name,
            "type": type(obj).__name__,
            "repr": str(obj)[:100],
        }


def print_dict_tree(d: Dict, indent: int = 0, max_indent: int = 10):
    """Pretty print dictionary tree structure."""
    if indent > max_indent:
        return

    prefix = "  " * indent

    if "name" in d:
        print(f"{prefix}├─ {d['name']}")
        indent += 1
        prefix = "  " * indent

    # Print type and basic info
    if "type" in d:
        print(f"{prefix}type: {d['type']}")

    # Print specific info based on type
    if d.get("type") == "torch.Tensor":
        print(f"{prefix}shape: {d['shape']}")
        print(f"{prefix}dtype: {d['dtype']}")
        print(f"{prefix}device: {d['device']}")
        if d.get("min") is not None:
            print(f"{prefix}range: [{d['min']:.4f}, {d['max']:.4f}]")
            print(f"{prefix}mean: {d['mean']:.4f}")

    elif d.get("type") == "np.ndarray":
        print(f"{prefix}shape: {d['shape']}")
        print(f"{prefix}dtype: {d['dtype']}")
        if d.get("min") is not None:
            print(f"{prefix}range: [{d['min']:.4f}, {d['max']:.4f}]")
            print(f"{prefix}mean: {d['mean']:.4f}")

    elif d.get("type") == "dict":
        print(f"{prefix}keys: {d.get('num_keys', 0)}")
        if "keys" in d:
            print(f"{prefix}key names: {d['keys']}")
        if "contents" in d:
            for value in d["contents"].values():
                print_dict_tree(value, indent + 1, max_indent)

    elif d.get("type") == "list":
        print(f"{prefix}length: {d.get('length', 0)}")
        if "sample_element" in d:
            print(f"{prefix}sample element:")
            print_dict_tree(d["sample_element"], indent + 1, max_indent)

    elif "value" in d:
        print(f"{prefix}value: {d['value']}")


def inspect_pickle_file(filepath: Path) -> None:
    """Inspect and print information about a pickle file."""
    print(f"\n{'='*80}")
    print(f"Inspecting: {filepath.name}")
    print(f"{'='*80}")
    print(f"File size: {filepath.stat().st_size / (1024*1024):.2f} MB")

    try:
        # Try loading with torch.load first (for torch.save files)
        try:
            with open(filepath, "rb") as f:
                data = torch.load(f, map_location=torch.device('cpu'), weights_only=False)
        except (RuntimeError, pickle.UnpicklingError) as e:
            # If CUDA device error, try with a custom unpickler that maps CUDA to CPU
            if "CUDA" in str(e):
                try:
                    import io
                    with open(filepath, "rb") as f:
                        buffer = io.BytesIO(f.read())
                    # Custom unpickler to force CPU mapping
                    class CPU_Unpickler(pickle.Unpickler):
                        def find_class(self, module, name):
                            if module == 'torch.storage' and name == '_load_from_bytes':
                                return lambda b: torch.load(io.BytesIO(b), map_location='cpu', weights_only=False)
                            return super().find_class(module, name)
                    data = CPU_Unpickler(buffer).load()
                except Exception:
                    # Fall back to regular pickle for standard pickle files
                    with open(filepath, "rb") as f:
                        data = pickle.load(f)
            else:
                # Fall back to regular pickle for standard pickle files
                with open(filepath, "rb") as f:
                    data = pickle.load(f)

        print(f"\nTop-level type: {type(data).__name__}")

        # Analyze the structure
        analysis = analyze_object(data, filepath.stem, depth=0, max_depth=4)

        print("\nStructure:")
        print_dict_tree(analysis, indent=0, max_indent=8)

        # Special handling for known explanation formats
        if isinstance(data, list) and len(data) > 0:
            print(f"\n{'─'*80}")
            print(f"List contains {len(data)} items")

            if isinstance(data[0], dict):
                print("\nFirst item keys:", list(data[0].keys()))

                # For explanation files, show some sample data
                if "triple" in data[0]:
                    print("\nSample triple explanation:")
                    print(f"  Triple: {data[0]['triple']}")
                    if "edge_mask" in data[0]:
                        mask = data[0]["edge_mask"]
                        if isinstance(mask, torch.Tensor):
                            print(f"  Edge mask shape: {mask.shape}")
                            print(f"  Edge mask range: [{mask.min().item():.4f}, {mask.max().item():.4f}]")
                            print(f"  Top 5 edge weights: {torch.topk(mask, min(5, len(mask))).values.tolist()}")

                    if "important_edges" in data[0]:
                        edges = data[0]["important_edges"]
                        print(f"  Number of important edges: {len(edges) if isinstance(edges, (list, np.ndarray, torch.Tensor)) else 'N/A'}")

        elif isinstance(data, dict):
            print(f"\n{'─'*80}")
            print("Dictionary keys:", list(data.keys()))

        print(f"\n{'='*80}\n")

    except Exception as e:
        print(f"Error loading pickle file: {e}")
        import traceback
        traceback.print_exc()


def main():
    """Main function to inspect all pickle files in the explanations directory."""

    # Default path
    default_path = Path("/Users/jchung/Documents/RENCI/everycure/experiments/Influence_estimate/gnnexplain/data/05_model_explanations")

    # Allow command-line override
    if len(sys.argv) > 1:
        explanations_dir = Path(sys.argv[1])
    else:
        explanations_dir = default_path

    if not explanations_dir.exists():
        print(f"Directory not found: {explanations_dir}")
        sys.exit(1)

    # Find all pickle files
    pickle_files = sorted(explanations_dir.glob("*.pkl"))

    if not pickle_files:
        print(f"No pickle files found in {explanations_dir}")
        sys.exit(1)

    print(f"Found {len(pickle_files)} pickle file(s)")

    # Inspect each file
    for pkl_file in pickle_files:
        inspect_pickle_file(pkl_file)

    print("\n" + "="*80)
    print("Summary:")
    print("="*80)
    for pkl_file in pickle_files:
        size_mb = pkl_file.stat().st_size / (1024*1024)
        print(f"  {pkl_file.name:40s} {size_mb:8.2f} MB")


if __name__ == "__main__":
    main()
